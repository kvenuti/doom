{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rap_db import *\n",
    "from rap_clean import *\n",
    "from rap_viz import line, verse_graph\n",
    "from nltk.stem import *\n",
    "from nltk import pos_tag\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from sklearn.model_selection import train_test_split as tr_ts_spl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doom = art_load(['Doom'])['Doom']\n",
    "chief = art_load(['Chief Keef'])['Chief Keef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#verse count for each\n",
    "print(len(doom.uniq_art_verses))\n",
    "print(len(chief.uniq_art_verses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class line_data():\n",
    "    def __init__(self, in_line_obj):\n",
    "        self.line_obj = in_line_obj    \n",
    "        self.gen_line_stem()\n",
    "        self.gen_line_metrics()\n",
    "\n",
    "    #want to run in multinomial and bernouli ways (one with frequency one with there not there binary)\n",
    "    def gen_line_stem(self):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words_stm = [stemmer.stem(w.lower()) for w in self.line_obj.words_as_strings if stemmer.stem(w.lower()) not in stopwords.words('english')]\n",
    "        self.all_stemmed_words = list(filter(None, words_stm))\n",
    "        self.unique_stemmed_words = set(self.all_stemmed_words)\n",
    "        \n",
    "    #may want to add stemming if accuracy shitty\n",
    "    def gen_line_metrics(self):\n",
    "        #get the vowel strings needed\n",
    "        ex_vwls = self.line_obj.vowel_sounds\n",
    "        nr_vwls = [v[:2] for v in ex_vwls]\n",
    "        brd_vwls = [v[:1] for v in ex_vwls]\n",
    "        #then vowel sounds for unique words, do it this way to not remake word objects\n",
    "        check = set()\n",
    "        ex_vwls_uniqs = []\n",
    "        for w in self.line_obj.word_objs:\n",
    "            if w.text.lower() not in check:\n",
    "                check = check|{w.text.lower()}\n",
    "                ex_vwls_uniqs.extend(list(zip(*w.matches))[1])\n",
    "        nr_vwls_uniqs = [v[:2] for v in ex_vwls_uniqs]\n",
    "        brd_vwls_uniqs = [v[:1] for v in ex_vwls_uniqs]\n",
    "        \n",
    "        #these are used a lot\n",
    "        wrds = self.line_obj.words_as_strings\n",
    "        unq_wrds = self.line_obj.uniq_words_as_strings\n",
    "        wrd_cnt = len(wrds)\n",
    "        unq_wrd_cnt = len(unq_wrds)\n",
    "        blobs = TextBlob(\" \".join(wrds)).sentiment\n",
    "        \n",
    "        #word based metrics\n",
    "        self.metrics={'avg_wrd_len':sum(map(len,wrds))/wrd_cnt,\n",
    "        'avg_unq_wrd_len':sum(map(len,unq_wrds))/unq_wrd_cnt,\n",
    "        'unq_wrds_rat':unq_wrd_cnt/wrd_cnt,\n",
    "                      \n",
    "        #vowel based metrics\n",
    "            #average vowel sounds per word\n",
    "        'avg_wrd_vwls':len(ex_vwls)/wrd_cnt,\n",
    "            #average vowel sounds per unique word\n",
    "        'avg_unq_wrd_vwls':len(ex_vwls_uniqs)/unq_wrd_cnt,\n",
    "            #average unique vowel sounds per word\n",
    "        'avg_wrd_brd_unq_vwls':len(set(brd_vwls))/wrd_cnt,\n",
    "        'avg_wrd_nr_unq_vwls':len(set(nr_vwls))/wrd_cnt,\n",
    "        'avg_wrd_ex_unq_vwls':len(set(ex_vwls))/wrd_cnt,\n",
    "            #average unique vowel sounds per unique word\n",
    "        'avg_unq_wrd_brd_unq_vwls':len(set(brd_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_nr_unq_vwls':len(set(nr_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_ex_unq_vwls':len(set(ex_vwls_uniqs))/unq_wrd_cnt,\n",
    "                      \n",
    "        #specialized metrics\n",
    "        'pol':blobs.polarity,\n",
    "        'subj':blobs.subjectivity,\n",
    "        'uniq_pos_rat': len(set(list(zip(*pos_tag(wrds)))[1]))/wrd_cnt,\n",
    "        'uniq_pos_unq_wrd_rat': len(set(list(zip(*pos_tag(unq_wrds)))[1]))/unq_wrd_cnt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def art_to_verse_graph(art_obj, inp_pop=False, inp_exc_line=True, inp_opto_type='near'):#opto stuff here\n",
    "    ret_verse_graphs = []\n",
    "    for s in art_obj.songs:\n",
    "        for v in s.uniq_art_verses:\n",
    "            verse_g = verse_graph(v, art_obj.name, s.name)\n",
    "            verse_g.opto_matches(pop=inp_pop, exc_line=inp_exc_line, opto_type=inp_opto_type, record=False)\n",
    "            ret_verse_graphs.append(verse_g)\n",
    "    return ret_verse_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verse_graph_to_lines(verse_graph_obj):\n",
    "    ret_lines = []\n",
    "    for v_line in verse_graph_obj.ver_as_lines:\n",
    "        if v_line.word_objs:\n",
    "            line_data_obj = line_data(v_line)\n",
    "            ret_lines.append(line_data_obj)\n",
    "    return ret_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_line_data(lines, art_name):\n",
    "    metric_df = pd.DataFrame(columns = list(lines[0].metrics.keys())+['artist'])\n",
    "    lingustic_df = pd.DataFrame(columns=['artist','text','unique_text'])\n",
    "    for l in lines:\n",
    "        app_dic = copy(l.metrics)\n",
    "        app_dic.update({'artist':art_name})\n",
    "        metric_df = metric_df.append(app_dic, ignore_index=True)\n",
    "        lingustic_df = lingustic_df.append({'text':l.all_stemmed_words, 'unique_text':l.unique_stemmed_words, 'artist':art_name}, ignore_index=True)\n",
    "    return metric_df, lingustic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_rapper_data(art_lines):\n",
    "    y = art_lines['artist']\n",
    "    x = art_lines.ix[:, art_lines.columns.difference(['artist'])]\n",
    "    x_tr, _x_ts, y_tr, _y_ts = tr_ts_spl(x, y, test_size=0.4, random_state=42)\n",
    "    x_ts, x_vl, y_ts, y_vl = tr_ts_spl(_x_ts, _y_ts, test_size=0.5, random_state=41)\n",
    "    return {'x_train':x_tr,'y_train':y_tr,'x_test':x_ts,'y_test':y_ts,'x_val':x_vl,'y_val':y_vl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2873  Doom lines.\n",
      "2012  Chief Keef lines.\n"
     ]
    }
   ],
   "source": [
    "#line_count\n",
    "all_doom_lines = [ver for verse_g in art_to_verse_graph(doom, inp_pop=2, inp_exc_line=False, inp_opto_type='exact') for ver in verse_graph_to_lines(verse_g)]\n",
    "print(len(all_doom_lines), \"Doom lines.\")\n",
    "all_chief_lines = [ver for verse_g in art_to_verse_graph(chief, inp_pop=False, inp_exc_line=True, inp_opto_type='near') for ver in verse_graph_to_lines(verse_g)]\n",
    "print(len(all_chief_lines), \"Chief Keef lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvenuti\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning:\n",
      "\n",
      "\n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#still need to combine\n",
    "doom_metrics, doom_language = create_line_data(all_doom_lines, 'Doom')\n",
    "doom_metric_data = make_rapper_data(doom_metrics)\n",
    "doom_language_data = make_rapper_data(doom_language)\n",
    "\n",
    "chief_metrics, chief_language = create_line_data(all_chief_lines, 'Chief Keef')\n",
    "chief_metric_data = make_rapper_data(chief_metrics)\n",
    "chief_language_data = make_rapper_data(chief_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67041965199590581"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_mod = SVC(degree=6, kernel='poly')\n",
    "svm_mod.fit(doom_metric_data['x_train'].append(chief_metric_data['x_train']), doom_metric_data['y_train'].append(chief_metric_data['y_train']))\n",
    "svm_mod.score(doom_metric_data['x_test'].append(chief_metric_data['x_test']), doom_metric_data['y_test'].append(chief_metric_data['y_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you did\n",
    "built basic data sorting functions, ran SVM with no tuning 67% accuracy\n",
    "\n",
    "\n",
    "What you need to do next\n",
    "clean functions, don't like their current layout (make more iterable for multi class)\n",
    "seet up lingusitc data set trainngi\n",
    "\n",
    "it's running the same iteration over and over. ideally making dataset shoukld only run through each graph, each line once. make functions individual not with for loops and then overall wrapper function\n",
    "\n",
    "Notes\n",
    "\n",
    "Long term\n",
    "Train models using two different training methadologies\n",
    "1. text bag of words (simply look at words in textand classify using a naive bayes, random forest, SVM)\n",
    "2. make a row for every line based on the whiteboarded lingustic measures (def use svm, maybe random forest, maybe KNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
