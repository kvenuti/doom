{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keenan/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from rap_db import *\n",
    "from rap_clean import *\n",
    "from rap_viz import line, verse_graph\n",
    "from nltk.stem import *\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doom = art_load(['Doom'])['Doom']\n",
    "chief = art_load(['Chief Keef'])['Chief Keef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#verse count for each\n",
    "print(len(doom.uniq_art_verses))\n",
    "print(len(chief.uniq_art_verses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class line_data():\n",
    "    def __init__(self, in_line_obj):\n",
    "        self.line_obj = in_line_obj    \n",
    "        self.gen_line_stem()\n",
    "        self.gen_line_metrics()\n",
    "\n",
    "    #want to run in multinomial and bernouli ways (one with frequency one with there not there binary)\n",
    "    def gen_line_stem(self):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words_stm = [stemmer.stem(w.lower()) for w in self.line_obj.words_as_strings if stemmer.stem(w.lower()) not in stopwords.words('english')]\n",
    "        self.all_stemmed_words = list(filter(None, words_stm))\n",
    "        self.unique_stemmed_words = set(self.all_stemmed_words)\n",
    "        \n",
    "    #may want to add stemming if accuracy shitty\n",
    "    def gen_line_metrics(self):\n",
    "        #get the vowel strings needed\n",
    "        ex_vwls = self.line_obj.vowel_sounds\n",
    "        nr_vwls = [v[:2] for v in ex_vwls]\n",
    "        brd_vwls = [v[:1] for v in ex_vwls]\n",
    "        #then vowel sounds for unique words, do it this way to not remake word objects\n",
    "        check = set()\n",
    "        ex_vwls_uniqs = []\n",
    "        for w in self.line_obj.word_objs:\n",
    "            if w.text.lower() not in check:\n",
    "                check = check|{w.text.lower()}\n",
    "                ex_vwls_uniqs.extend(list(zip(*w.matches))[1])\n",
    "        nr_vwls_uniqs = [v[:2] for v in ex_vwls_uniqs]\n",
    "        brd_vwls_uniqs = [v[:1] for v in ex_vwls_uniqs]\n",
    "        \n",
    "        #these are used a lot\n",
    "        wrds = self.line_obj.words_as_strings\n",
    "        unq_wrds = self.line_obj.uniq_words_as_strings\n",
    "        wrd_cnt = len(wrds)\n",
    "        unq_wrd_cnt = len(unq_wrds)\n",
    "        blobs = TextBlob(\" \".join(wrds)).sentiment\n",
    "        \n",
    "        #word based metrics\n",
    "        self.metrics={'avg_wrd_len':sum(map(len,wrds))/wrd_cnt,\n",
    "        'avg_unq_wrd_len':sum(map(len,unq_wrds))/unq_wrd_cnt,\n",
    "        'unq_wrds_rat':unq_wrd_cnt/wrd_cnt,\n",
    "                      \n",
    "        #vowel based metrics\n",
    "            #average vowel sounds per word\n",
    "        'avg_wrd_vwls':len(ex_vwls)/wrd_cnt,\n",
    "            #average vowel sounds per unique word\n",
    "        'avg_unq_wrd_vwls':len(ex_vwls_uniqs)/unq_wrd_cnt,\n",
    "            #average unique vowel sounds per word\n",
    "        'avg_wrd_brd_unq_vwls':len(set(brd_vwls))/wrd_cnt,\n",
    "        'avg_wrd_nr_unq_vwls':len(set(nr_vwls))/wrd_cnt,\n",
    "        'avg_wrd_ex_unq_vwls':len(set(ex_vwls))/wrd_cnt,\n",
    "            #average unique vowel sounds per unique word\n",
    "        'avg_unq_wrd_brd_unq_vwls':len(set(brd_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_nr_unq_vwls':len(set(nr_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_ex_unq_vwls':len(set(ex_vwls_uniqs))/unq_wrd_cnt,\n",
    "                      \n",
    "        #specialized metrics\n",
    "        'pol':blobs.polarity,\n",
    "        'subj':blobs.subjectivity,\n",
    "        'uniq_pos_rat': len(set(list(zip(*pos_tag(wrds)))[1]))/wrd_cnt,\n",
    "        'uniq_pos_unq_wrd_rat': len(set(list(zip(*pos_tag(unq_wrds)))[1]))/unq_wrd_cnt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_to_verse_graph(art_obj, inp_pop=False, inp_exc_line=True, inp_opto_type='near'):#opto stuff here\n",
    "    ret_verse_graphs = []\n",
    "    for s in art_obj.songs:\n",
    "        for v in s.uniq_art_verses:\n",
    "            verse_g = verse_graph(v, art_obj.name, s.name)\n",
    "            verse_g.opto_matches(pop=inp_pop, exc_line=inp_exc_line, opto_type=inp_opto_type, record=False)\n",
    "            ret_verse_graphs.append(verse_g)\n",
    "    return ret_verse_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verse_graph_to_lines(verse_graph_obj):\n",
    "    ret_lines = []\n",
    "    for v_line in verse_graph_obj.ver_as_lines:\n",
    "        if v_line.word_objs:\n",
    "            line_data_obj = line_data(v_line)\n",
    "            ret_lines.append(line_data_obj)\n",
    "    return ret_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2873"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line_count\n",
    "all_doom_lines = [ver for verse_g in art_to_verse_graph(doom, inp_pop=2, inp_exc_line=False, inp_opto_type='exact') for ver in verse_graph_to_lines(verse_g)]\n",
    "len(all_doom_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chief_lines = [ver for verse_g in art_to_verse_graph(chief, inp_pop=False, inp_exc_line=True, inp_opto_type='near') for ver in verse_graph_to_lines(verse_g)]\n",
    "len(all_chief_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you did\n",
    "both could always use work\n",
    "built stemmer\n",
    "built full metrics method\n",
    "\n",
    "What you need to do next\n",
    "\n",
    "Notes\n",
    "\n",
    "Long term\n",
    "Train models using two different training methadologies\n",
    "1. text bag of words (simply look at words in textand classify using a naive bayes, random forest, SVM)\n",
    "2. make a row for every line based on the whiteboarded lingustic measures (def use svm, maybe random forest, maybe KNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
