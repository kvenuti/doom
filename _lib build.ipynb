{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keenan/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from rap_db import *\n",
    "from rap_clean import *\n",
    "from rap_viz import line, verse_graph\n",
    "from nltk.stem import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doom = art_load(['Doom'])['Doom']\n",
    "chief = art_load(['Chief Keef'])['Chief Keef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#verse count for each\n",
    "print(len(doom.uniq_art_verses))\n",
    "print(len(chief.uniq_art_verses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE LINE METRICS (check for correlations with corr plot)\n",
    "2. Unique words ratio (#uniq words/#of words) done\n",
    "3. Avg word length sum(#of letters in all words)/(#all words) done\n",
    "4. Avg unique word length sum(#of letters in all unqiue words)/(#all unique words) done\n",
    "5. Polarity\n",
    "6. Sentiment\n",
    "7. Unique broad vowel sound ratio (#of unique broad sylbls/# of broad sylbls)\n",
    "8. Unique near vowel sound ratio (#of unique near sylbls/# of near sylbls)\n",
    "9. Unique exact vowel sound ratio (#of unique exact sylbls/# of exact sylbls)\n",
    "10. Avg sylbls per word sum(#of sylbs in all words)/(#all words)\n",
    "11. Avg unqiue sylbls per word sum(#of unique sylbs in all words)/(#all unique words)\n",
    "12. Unique POS Ratio (#of unique POS)/(#all words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class line_data():\n",
    "    #mainly just assign attributes used in other methods\n",
    "    def __init__(self, line_obj):\n",
    "        #words\n",
    "        self.all_word_objs = line_obj.word_objs\n",
    "        self.all_word_strs = line_obj.words_as_strings\n",
    "        self.all_uniq_word_strs = line_obj.uniq_words_as_strings\n",
    "        #vowels\n",
    "        self.ex_vwls = line_obj.vowel_sounds\n",
    "        self.nr_vwls = [v[:2] for v in self.ex_vwls]\n",
    "        self.brd_vwls = [v[:1] for v in self.ex_vwls]\n",
    "        #this is going to add sometime but need to help with perfectly correlated features\n",
    "        check = set()\n",
    "        self.ex_vwls_uniqs = []\n",
    "        for w in self.all_word_objs:\n",
    "            if w.text.lower() not in check:\n",
    "                check = check|{w.text.lower()}\n",
    "                self.ex_vwls_uniqs.extend(list(zip(*w.matches))[1])\n",
    "        self.nr_vwls_uniqs = [v[:2] for v in self.ex_vwls_uniqs]\n",
    "        self.brd_vwls_uniqs = [v[:1] for v in self.ex_vwls_uniqs]\n",
    "        \n",
    "        self.gen_line_stem()\n",
    "        self.gen_line_metrics()\n",
    "                \n",
    "        #want to run in multinomial and bernouli ways (one with frequency one with there not there binary)\n",
    "    def gen_line_stem(self):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words_stm = [stemmer.stem(w.lower()) for w in self.all_word_strs if stemmer.stem(w.lower()) not in stopwords.words('english')]\n",
    "        self.all_stemmed_words = list(filter(None, words_stm))\n",
    "        self.unique_stemmed_words = set(self.all_stemmed_words)\n",
    "        \n",
    "    #may want to add stemming if accuracy shitty\n",
    "    def gen_line_metrics(self):\n",
    "        #word based metrics\n",
    "        self.metrics={'avg_wrd_len':sum(map(len,self.all_word_strs))/len(self.all_word_strs),\n",
    "        'avg_unq_wrd_len':sum(map(len,self.all_uniq_word_strs))/len(self.all_uniq_word_strs),\n",
    "        'unq_wrds_rat':len(self.all_uniq_word_strs)/len(self.all_word_strs),\n",
    "                      \n",
    "        #vowel based metrics\n",
    "            #average vowel sounds per word\n",
    "        'avg_wrd_vwls':len(self.ex_vwls)/len(self.all_word_strs),\n",
    "            #average unique vowel sounds per word\n",
    "        'avg_wrd_brd_unq_vwls':len(set(self.brd_vwls))/len(self.all_word_strs),\n",
    "        'avg_wrd_nr_unq_vwls':len(set(self.nr_vwls))/len(self.all_word_strs),\n",
    "        'avg_wrd_ex_unq_vwls':len(set(self.ex_vwls))/len(self.all_word_strs),\n",
    "            #average unique vowel sounds per unique word\n",
    "        'avg_unq_wrd_brd_unq_vwls':len(set(self.brd_vwls_uniqs))/len(self.all_uniq_word_strs),\n",
    "        'avg_unq_wrd_nr_unq_vwls':len(set(self.nr_vwls_uniqs))/len(self.all_uniq_word_strs),\n",
    "        'avg_unq_wrd_ex_unq_vwls':len(set(self.ex_vwls_uniqs))/len(self.all_uniq_word_strs),\n",
    "                      \n",
    "        #specialized metrics\n",
    "        'pol':float(),\n",
    "        'sen':float(),\n",
    "        'uniq_pos_rat':float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_to_verse_graph(art_obj, inp_pop=False, inp_exc_line=True, inp_opto_type='near'):#opto stuff here\n",
    "    ret_verse_graphs = []\n",
    "    for s in art_obj.songs:\n",
    "        for v in s.uniq_art_verses:\n",
    "            verse_g = verse_graph(v, art_obj.name, s.name)\n",
    "            verse_g.opto_matches(pop=inp_pop, exc_line=inp_exc_line, opto_type=inp_opto_type, record=False)\n",
    "            ret_verse_graphs.append(verse_g)\n",
    "    return ret_verse_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verse_graph_to_lines(verse_graph_obj):\n",
    "    ret_lines = []\n",
    "    for v_line in verse_graph_obj.ver_as_lines:\n",
    "        if v_line.word_objs:\n",
    "            line_data_obj = line_data(v_line)\n",
    "            ret_lines.append(line_data_obj)\n",
    "    return ret_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2873"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line_count\n",
    "all_doom_lines = [ver for verse_g in art_to_verse_graph(doom, inp_pop=2, inp_exc_line=False, inp_opto_type='exact') for ver in verse_graph_to_lines(verse_g)]\n",
    "len(all_doom_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chief_lines = [ver for verse_g in art_to_verse_graph(chief, inp_pop=False, inp_exc_line=True, inp_opto_type='near') for ver in verse_graph_to_lines(verse_g)]\n",
    "len(all_chief_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you did\n",
    "built stemmer\n",
    "\n",
    "What you need to do next\n",
    "\n",
    "\n",
    "Notes\n",
    "each artist should have a list of rap_line objects. Each line should have a word object dictionary, the preserved words as list of strings and then stemmed words/with unique stuff\n",
    "\n",
    "Long term\n",
    "Train models using two different training methadologies\n",
    "1. text bag of words (simply look at words in textand classify using a naive bayes, random forest, SVM)\n",
    "2. make a row for every line based on the whiteboarded lingustic measures (def use svm, maybe random forest, maybe KNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
